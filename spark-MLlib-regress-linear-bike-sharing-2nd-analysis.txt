	- instant: record index
	- dteday : date
	- season : season (1:springer, 2:summer, 3:fall, 4:winter)
	- yr : year (0: 2011, 1:2012)
	- mnth : month ( 1 to 12)
	- hr : hour (0 to 23)
	- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
	- weekday : day of the week
	- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
	+ weathersit : 
		- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
		- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
		- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
		- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
	- temp : Normalized temperature in Celsius. The values are divided to 41 (max)
	- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)
	- hum: Normalized humidity. The values are divided to 100 (max)
	- windspeed: Normalized wind speed. The values are divided to 67 (max)
	- casual: count of casual users
	- registered: count of registered users
	- cnt: count of total rental bikes including both casual and registered
	
case class BikeRec (
  instant : Long,
  dteday : String,
  season : Int,
  yr : Int,
  mnth : Int,
  hr: Int,
  holiday : Int,
  weekday : Int,
  workingday : Int,
  weathersit : Int,
  atemp : Double,
  hum : Double,
  windspeed : Double,
  casual : Double,
  registered : Double,
  cnt : Double
)

val dfrdd = rdd.map( x => BikeRec(x(0).toLong,x(1),x(2).toInt,x(3).toInt,x(4).toInt,x(5).toInt,x(6).toInt,x(7).toInt,x(8).toInt,x(9).toInt,
                          x(10).toDouble,x(11).toDouble,x(12).toDouble,x(13).toDouble,x(14).toDouble,x(15).toDouble))
						  
val df = dfrdd.toDF

df.write.saveAsTable("bike_share")

spark.sql("select dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,avg(atemp),avg(hum),avg(windspeed),sum(cnt) from bike_share group by dteday,season,yr,mnth,holiday,weekday,workingday,weathersit order by dteday").show

val rdd = spark.sql("select dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,avg(atemp),avg(hum),avg(windspeed),sum(cnt) from bike_share group by dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit order by dteday").rdd

val rdd1 = rdd.map(r => r.toSeq.toArray.map( x => x.toString))

val rdd2 = rdd1.map( x => {
  x(4) = x(4) match {
    case "6"|"7"|"8"|"9" => "rush-morning"
    case "10"|"11"|"12"|"13"|"14"|"15"|"16" => "lunch"
    case "17"|"18"|"19" => "rush-afternoon"
    case "20"|"21"|"22" => "evening"
    case "23"|"0" => "mid-night"
    case "1"|"2"|"3"|"4"|"5" => "night"
  }
  x
})

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd2,1,4,6,8)

val rdd2 = rdd1.map(x => Array(x(5).toDouble,x(7).toDouble,x(9).toDouble,x(10).toDouble,x(11).toDouble,x(12).toDouble))
 
val vect = concat.zip(rdd2).map(x => (x._1.toList ++ x._2.toList).toArray)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(r => {
   val arr_size = r.size
   val l = r(arr_size-1)
   val f = r.slice(0,arr_size-1)
   LabeledPoint(l,Vectors.dense(f))
 })
 
data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- step-size = 1.0
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(100)
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)
model: intercept = 41.233292338183496, numFeatures = 26

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res28: Array[(Double, Double)] = Array((-61.57412730822126,32.0), (-57.45392501400317,10.0), (-57.45392501400317,1.0), (111.52520893563357,2.0), (102.39735754862579,55.0), (189.27285492709953,31.0), (2.1786463575383905,13.0), (240.30762556591858,29.0), (143.90699640811476,63.0), (117.00008288878595,41.0))

---- step-size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
model: intercept = 42.957001586578166, numFeatures = 26
 
val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res30: Array[(Double, Double)] = Array((65.98597960636388,32.0), (66.44550440387476,10.0), (66.44550440387476,1.0), (120.0207412429302,2.0), (131.2007970395208,55.0), (121.01256107086664,31.0), (84.04496542191842,13.0), (141.02294293750452,29.0), (143.7799539477519,63.0), (145.81799646371496,41.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 134.74076821434366
validMetrics.meanSquaredError  // 18155.074618991486

---- step-size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
model: intercept = 21.797269148115404, numFeatures = 26
 
val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res36: Array[(Double, Double)] = Array((55.915227864308825,32.0), (55.74748247100708,10.0), (55.74748247100708,1.0), (61.920107451204544,2.0), (67.78511635430979,55.0), (54.78579185084989,31.0), (53.284071766106386,13.0), (60.3325349067386,29.0), (71.72052675077906,63.0), (74.1765448831356,41.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 173.36596285076993
validMetrics.meanSquaredError  // 30055.757075174537

---- step-size = 0.001
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
model: intercept = 3.735501772574464, numFeatures = 26
 
val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res40: Array[(Double, Double)] = Array((8.435223816301708,32.0), (8.409588203659942,10.0), (8.409588203659942,1.0), (9.033145521822195,2.0), (9.802161265328706,55.0), (8.029846720702498,31.0), (7.980581397618435,13.0), (8.71861041910464,29.0), (10.302663356283684,63.0), (10.633724728658336,41.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 214.0162341538308
validMetrics.meanSquaredError  // 45802.94848138733
