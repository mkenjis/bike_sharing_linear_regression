---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("bike/hour.csv").filter( x => ! x.contains("instant")).map(x => x.split(","))

rdd.take(5)
res2: Array[Array[String]] = Array(Array(1, 2011-01-01, 1, 0, 1, 0, 0, 6, 0, 1, 0.24, 0.2879, 0.81, 0, 3, 13, 16), Array(2, 2011-01-01, 1, 0, 1, 1, 0, 6, 0, 1, 0.22, 0.2727, 0.8, 0, 8, 32, 40), Array(3, 2011-01-01, 1, 0, 1, 2, 0, 6, 0, 1, 0.22, 0.2727, 0.8, 0, 5, 27, 32), Array(4, 2011-01-01, 1, 0, 1, 3, 0, 6, 0, 1, 0.24, 0.2879, 0.75, 0, 3, 10, 13), Array(5, 2011-01-01, 1, 0, 1, 4, 0, 6, 0, 1, 0.24, 0.2879, 0.75, 0, 0, 1, 1))

---- Convert season(2), hr(5), weekday(7), weathersit(9) as 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,2,5,7,9)

val rdd1 = rdd.map(x => Array(x(16).toDouble,x(6).toDouble,x(8).toDouble,x(10).toDouble,x(11).toDouble,x(12).toDouble,x(13).toDouble))

val vect = rdd1.zip(concat).map(x => (x._1.toList ++ x._2.toList).toArray)

vect.take(5)
res3: Array[Array[Double]] = Array(Array(16.0, 0.0, 0.0, 0.24, 0.2879, 0.81, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0), Array(40.0, 0.0, 0.0, 0.22, 0.2727, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0), Array(32.0, 0.0, 0.0, 0.22, 0.2727, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0), Array(13.0, 0.0, 0.0, 0.24, 0.2879,...

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(x => {
   val l = x(0).toDouble
   val f = x.slice(1, x.size-1)
   LabeledPoint(l, Vectors.dense(f))
 })

data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)


---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)
model: intercept = 78.71448830815403, numFeatures = 44

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res8: Array[(Double, Double)] = Array((110.99490216859101,36.0), (190.0118184150961,93.0), (210.6791875985857,35.0), (132.32924103761815,36.0), (98.16146303503986,34.0), (61.888287757568676,39.0), (141.26862663721994,93.0), (132.808474285326,75.0), (277.47874966333006,53.0), (100.82068432776525,31.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError   // 115.46666623231461
validMetrics.meanSquaredError       // 13332.551010804742

------- Transforming the label data --------------

import java.lang.Math._
val trainSet_lg = trainSet.map{ case LabeledPoint(x,y) => LabeledPoint(Math.log(x), y) }
val testSet_lg = testSet.map{ case LabeledPoint(x,y) => LabeledPoint(Math.log(x), y) }

---- MLlib Linear regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet_lg)
model: intercept = 2.6050312517953813, numFeatures = 44

val validPredicts = testSet_lg.map(x => (Math.exp(model.predict(x.features)),Math.exp(x.label)))

validPredicts.take(10)
res15: Array[(Double, Double)] = Array((87.84776374759001,36.0), (131.43045823685435,93.00000000000003), (96.02348763453185,34.99999999999999), (104.52110593325693,36.0), (87.12763025477597,34.00000000000001), (67.74588434611948,38.99999999999999), (97.48077784485073,93.00000000000003), (88.82843309132312,74.99999999999997), (154.78089050149825,53.00000000000001), (72.27399298319287,31.0))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError   // 127.08541595611491
validMetrics.meanSquaredError       // 16150.702948738746

